{
  "name": "Local Drive Document Indexer",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "hours",
              "hoursInterval": 6
            }
          ]
        }
      },
      "id": "schedule-trigger",
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "position": [250, 300],
      "typeVersion": 1.1,
      "notes": "Runs every 6 hours to index new/updated documents"
    },
    {
      "parameters": {
        "functionCode": "// Configuration - MODIFY THESE FOR YOUR ENVIRONMENT\nconst config = {\n  // Base path to scan (use double backslashes for Windows)\n  basePath: 'G:\\\\',  // Change to your mapped drive or local folder\n  \n  // Specific subdirectories to scan (leave empty to scan entire drive)\n  // Example: ['Policies', 'Procedures', 'Documentation']\n  subdirectories: [],\n  \n  // File patterns to include\n  includePatterns: ['*.pdf', '*.docx', '*.txt', '*.md', '*.doc'],\n  \n  // Patterns to exclude\n  excludePatterns: ['~$*', '.*', 'Thumbs.db', '*.tmp'],\n  \n  // Maximum file size in MB (skip larger files)\n  maxFileSizeMB: 50,\n  \n  // Skip files older than X days (0 = no limit)\n  maxAgeInDays: 0\n};\n\n// Import required modules\nconst fs = require('fs').promises;\nconst path = require('path');\n\n// Function to recursively find files\nasync function findFiles(dir, fileList = []) {\n  try {\n    const files = await fs.readdir(dir, { withFileTypes: true });\n    \n    for (const file of files) {\n      const filePath = path.join(dir, file.name);\n      \n      if (file.isDirectory()) {\n        // Skip system directories\n        if (!file.name.startsWith('.') && !file.name.startsWith('$')) {\n          await findFiles(filePath, fileList);\n        }\n      } else {\n        // Check if file matches patterns\n        const ext = path.extname(file.name).toLowerCase();\n        const shouldInclude = config.includePatterns.some(pattern => {\n          const regex = new RegExp(pattern.replace('*', '.*'));\n          return regex.test(file.name);\n        });\n        \n        const shouldExclude = config.excludePatterns.some(pattern => {\n          const regex = new RegExp(pattern.replace('*', '.*'));\n          return regex.test(file.name);\n        });\n        \n        if (shouldInclude && !shouldExclude) {\n          try {\n            const stats = await fs.stat(filePath);\n            \n            // Check file size\n            const fileSizeMB = stats.size / (1024 * 1024);\n            if (fileSizeMB > config.maxFileSizeMB) {\n              console.log(`Skipping large file: ${file.name} (${fileSizeMB.toFixed(2)} MB)`);\n              continue;\n            }\n            \n            // Check file age if configured\n            if (config.maxAgeInDays > 0) {\n              const ageInDays = (Date.now() - stats.mtime) / (1000 * 60 * 60 * 24);\n              if (ageInDays > config.maxAgeInDays) {\n                continue;\n              }\n            }\n            \n            fileList.push({\n              filepath: filePath,\n              filename: file.name,\n              directory: dir,\n              extension: ext,\n              size: stats.size,\n              modified: stats.mtime,\n              created: stats.birthtime\n            });\n          } catch (err) {\n            console.error(`Error accessing file ${filePath}: ${err.message}`);\n          }\n        }\n      }\n    }\n  } catch (err) {\n    console.error(`Error reading directory ${dir}: ${err.message}`);\n  }\n  \n  return fileList;\n}\n\n// Main execution\ntry {\n  let scanPaths = [];\n  \n  if (config.subdirectories && config.subdirectories.length > 0) {\n    // Scan specific subdirectories\n    scanPaths = config.subdirectories.map(subdir => \n      path.join(config.basePath, subdir)\n    );\n  } else {\n    // Scan entire base path\n    scanPaths = [config.basePath];\n  }\n  \n  const allFiles = [];\n  \n  for (const scanPath of scanPaths) {\n    console.log(`Scanning: ${scanPath}`);\n    const files = await findFiles(scanPath);\n    allFiles.push(...files);\n  }\n  \n  console.log(`Found ${allFiles.length} files to index`);\n  \n  // Return files as items for next node\n  return allFiles.map(file => ({\n    json: {\n      filepath: file.filepath,\n      filename: file.filename,\n      directory: file.directory,\n      extension: file.extension,\n      size: file.size,\n      modified: file.modified,\n      created: file.created,\n      fileId: Buffer.from(file.filepath).toString('base64') // Create unique ID\n    }\n  }));\n  \n} catch (error) {\n  throw new Error(`Failed to scan files: ${error.message}`);\n}"
      },
      "id": "scan-local-files",
      "name": "Scan Local Files",
      "type": "n8n-nodes-base.function",
      "position": [450, 300],
      "typeVersion": 1,
      "notes": "Scans G: drive or specified local folders for documents"
    },
    {
      "parameters": {
        "filePath": "={{$json.filepath}}",
        "dataPropertyName": "fileContent"
      },
      "id": "read-file",
      "name": "Read File",
      "type": "n8n-nodes-base.readBinaryFile",
      "position": [650, 300],
      "typeVersion": 1,
      "notes": "Reads file content from local filesystem"
    },
    {
      "parameters": {
        "functionCode": "// Extract text based on file type\nconst filepath = $input.item.json.filepath;\nconst filename = $input.item.json.filename;\nconst extension = $input.item.json.extension.toLowerCase();\nconst fileContent = $input.item.binary.fileContent;\n\nlet textContent = '';\n\ntry {\n  if (extension === '.txt' || extension === '.md') {\n    // Plain text files - direct conversion\n    textContent = Buffer.from(fileContent.data, 'base64').toString('utf-8');\n    \n  } else if (extension === '.pdf') {\n    // PDF files - requires pdf-parse library\n    // For workshop, we'll do basic extraction or skip\n    // In production, use pdf-parse or similar\n    console.log(`PDF processing: ${filename}`);\n    // Simplified - you'd need proper PDF parsing here\n    textContent = `[PDF Content from ${filename} - requires pdf-parse library for full extraction]`;\n    \n  } else if (extension === '.docx' || extension === '.doc') {\n    // Word documents - requires mammoth or similar\n    console.log(`Word processing: ${filename}`);\n    // Simplified - you'd need proper DOCX parsing here\n    textContent = `[Word Content from ${filename} - requires mammoth library for full extraction]`;\n    \n    // Basic extraction for DOCX (XML-based)\n    if (extension === '.docx') {\n      try {\n        // DOCX files are zipped XML - this is a simplified extraction\n        const content = Buffer.from(fileContent.data, 'base64').toString('utf-8');\n        // Extract readable text between XML tags (very basic)\n        const matches = content.match(/>([^<]+)</g);\n        if (matches) {\n          textContent = matches.map(m => m.slice(1, -1)).join(' ');\n        }\n      } catch (e) {\n        console.log('Basic DOCX extraction failed, needs proper library');\n      }\n    }\n  } else {\n    // Try to read as text for unknown types\n    try {\n      textContent = Buffer.from(fileContent.data, 'base64').toString('utf-8');\n    } catch (e) {\n      textContent = `[Unable to extract text from ${filename}]`;\n    }\n  }\n  \n  // Clean the extracted text\n  textContent = textContent\n    .replace(/\\s+/g, ' ')  // Normalize whitespace\n    .replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]/g, '')  // Remove control characters\n    .trim();\n  \n  // Skip if no meaningful content\n  if (textContent.length < 50) {\n    console.log(`Skipping ${filename} - insufficient content`);\n    return [];\n  }\n  \n  return {\n    filepath: filepath,\n    filename: filename,\n    fileId: $input.item.json.fileId,\n    directory: $input.item.json.directory,\n    extension: extension,\n    modified: $input.item.json.modified,\n    textContent: textContent,\n    contentLength: textContent.length\n  };\n  \n} catch (error) {\n  console.error(`Error processing ${filename}: ${error.message}`);\n  return [];\n}"
      },
      "id": "extract-text",
      "name": "Extract Text",
      "type": "n8n-nodes-base.function",
      "position": [850, 300],
      "typeVersion": 1,
      "notes": "Extracts text from various file formats"
    },
    {
      "parameters": {
        "functionCode": "// Chunk text into smaller segments with overlap\nconst text = $input.item.json.textContent;\nconst filepath = $input.item.json.filepath;\nconst filename = $input.item.json.filename;\nconst fileId = $input.item.json.fileId;\nconst modified = $input.item.json.modified;\n\n// Configuration\nconst CHUNK_SIZE = 1000; // characters\nconst CHUNK_OVERLAP = 200; // overlap between chunks\nconst MIN_CHUNK_SIZE = 100; // minimum chunk size\n\nfunction createChunks(text, chunkSize, overlap) {\n  const chunks = [];\n  let start = 0;\n  \n  while (start < text.length) {\n    const end = Math.min(start + chunkSize, text.length);\n    const chunk = text.slice(start, end);\n    \n    // Skip very small chunks at the end\n    if (chunk.length >= MIN_CHUNK_SIZE || chunks.length === 0) {\n      chunks.push({\n        text: chunk,\n        start_char: start,\n        end_char: end,\n        chunk_index: chunks.length\n      });\n    }\n    \n    // Move to next chunk with overlap\n    start += chunkSize - overlap;\n    \n    // Prevent infinite loop\n    if (end === text.length) break;\n  }\n  \n  return chunks;\n}\n\n// Create chunks\nconst chunks = createChunks(text, CHUNK_SIZE, CHUNK_OVERLAP);\n\nconsole.log(`Split ${filename} into ${chunks.length} chunks`);\n\n// Return array of chunk objects\nreturn chunks.map(chunk => ({\n  json: {\n    chunk_text: chunk.text,\n    chunk_index: chunk.chunk_index,\n    total_chunks: chunks.length,\n    metadata: {\n      source_file: filename,\n      file_path: filepath,\n      file_id: fileId,\n      chunk_start: chunk.start_char,\n      chunk_end: chunk.end_char,\n      modified_date: modified,\n      indexed_at: new Date().toISOString()\n    }\n  }\n}));"
      },
      "id": "chunk-text",
      "name": "Chunk Text",
      "type": "n8n-nodes-base.function",
      "position": [1050, 300],
      "typeVersion": 1,
      "notes": "Splits text into overlapping chunks for better retrieval"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:11434/api/embeddings",
        "options": {
          "bodyContentType": "json",
          "timeout": 30000
        },
        "bodyParametersJson": "{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"={{$json.chunk_text}}\"\n}"
      },
      "id": "generate-embeddings",
      "name": "Generate Embeddings",
      "type": "n8n-nodes-base.httpRequest",
      "position": [1250, 300],
      "typeVersion": 3,
      "notes": "Generates embeddings using Ollama"
    },
    {
      "parameters": {
        "functionCode": "// Prepare data for ChromaDB\nconst embedding = $input.item.json.embedding;\nconst metadata = $input.item.json.metadata;\nconst chunkText = $input.item.json.chunk_text;\nconst chunkId = `${metadata.file_id}_chunk_${$input.item.json.chunk_index}`;\n\nreturn {\n  id: chunkId,\n  embedding: embedding,\n  document: chunkText,\n  metadata: {\n    source: metadata.source_file,\n    path: metadata.file_path,\n    chunk_index: $input.item.json.chunk_index,\n    total_chunks: $input.item.json.total_chunks,\n    modified: metadata.modified_date,\n    indexed_at: metadata.indexed_at\n  }\n};"
      },
      "id": "prepare-chromadb",
      "name": "Prepare for ChromaDB",
      "type": "n8n-nodes-base.function",
      "position": [1450, 300],
      "typeVersion": 1,
      "notes": "Formats data for ChromaDB storage"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:8000/api/v1/collections/local_docs/add",
        "options": {
          "bodyContentType": "json",
          "timeout": 10000
        },
        "bodyParametersJson": "{\n  \"ids\": [\"={{$json.id}}\"],\n  \"embeddings\": [{{$json.embedding}}],\n  \"documents\": [\"={{$json.document}}\"],\n  \"metadatas\": [{{JSON.stringify($json.metadata)}}]\n}"
      },
      "id": "store-chromadb",
      "name": "Store in ChromaDB",
      "type": "n8n-nodes-base.httpRequest",
      "position": [1650, 300],
      "typeVersion": 3,
      "notes": "Stores embeddings and metadata in ChromaDB"
    }
  ],
  "connections": {
    "Schedule Trigger": {
      "main": [[{"node": "Scan Local Files"}]]
    },
    "Scan Local Files": {
      "main": [[{"node": "Read File"}]]
    },
    "Read File": {
      "main": [[{"node": "Extract Text"}]]
    },
    "Extract Text": {
      "main": [[{"node": "Chunk Text"}]]
    },
    "Chunk Text": {
      "main": [[{"node": "Generate Embeddings"}]]
    },
    "Generate Embeddings": {
      "main": [[{"node": "Prepare for ChromaDB"}]]
    },
    "Prepare for ChromaDB": {
      "main": [[{"node": "Store in ChromaDB"}]]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "meta": {
    "templateId": "local-document-indexer",
    "templateVersion": "1.0.0",
    "templateDescription": "Indexes documents from local/mapped drives for RAG Q&A"
  }
}