{
  "name": "G: Drive Document Indexer (Simplified HTTP)",
  "nodes": [
    {
      "parameters": {},
      "id": "manual-trigger",
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "position": [220, 300],
      "typeVersion": 1
    },
    {
      "parameters": {
        "command": "find /data/gdrive -type f \\( -name '*.txt' -o -name '*.md' -o -name '*.doc' -o -name '*.docx' -o -name '*.pdf' \\) -printf '%p|%s|%TY-%Tm-%Td %TH:%TM:%TS\\n' 2>/dev/null | head -100"
      },
      "id": "list-files",
      "name": "List Files in G: Drive",
      "type": "n8n-nodes-base.executeCommand",
      "position": [420, 300],
      "typeVersion": 1,
      "notes": "Scans /data/gdrive for documents. Adjust 'head -100' to change file limit."
    },
    {
      "parameters": {
        "jsCode": "// Parse the file list from the command output\nconst commandOutput = $input.first().json.stdout;\n\nif (!commandOutput || commandOutput.trim() === '') {\n  return [{\n    json: {\n      error: 'No files found in /data/gdrive',\n      suggestion: 'Check if G: drive is properly mounted to /data/gdrive'\n    }\n  }];\n}\n\n// Parse the file list\nconst files = [];\nconst lines = commandOutput.trim().split('\\n');\n\nfor (const line of lines) {\n  if (!line) continue;\n  \n  const parts = line.split('|');\n  if (parts.length >= 3) {\n    const filepath = parts[0];\n    const size = parseInt(parts[1]);\n    const modified = parts[2].split('.')[0]; // Remove decimal seconds\n    \n    // Get filename and extension\n    const filename = filepath.split('/').pop();\n    const extension = '.' + filename.split('.').pop().toLowerCase();\n    \n    // Skip files larger than 5MB\n    if (size > 5 * 1024 * 1024) {\n      console.log(`Skipping large file: ${filename} (${(size / 1024 / 1024).toFixed(2)} MB)`);\n      continue;\n    }\n    \n    files.push({\n      filepath: filepath,\n      filename: filename,\n      extension: extension,\n      size: size,\n      sizeMB: (size / 1024 / 1024).toFixed(2),\n      modified: modified,\n      displayPath: filepath.replace('/data/gdrive', 'G:')\n    });\n  }\n}\n\nconsole.log(`Found ${files.length} files to process`);\n\n// Return each file as a separate item for parallel processing\nreturn files.map(file => ({ json: file }));"
      },
      "id": "parse-file-list",
      "name": "Parse File List",
      "type": "n8n-nodes-base.code",
      "position": [620, 300],
      "typeVersion": 2
    },
    {
      "parameters": {
        "command": "=cat \"{{ $json.filepath }}\" | head -c 100000"
      },
      "id": "read-file-content",
      "name": "Read File Content",
      "type": "n8n-nodes-base.executeCommand",
      "position": [820, 300],
      "typeVersion": 1,
      "notes": "Reads up to 100KB of each file",
      "continueOnFail": true
    },
    {
      "parameters": {
        "jsCode": "// Process all files and create chunks with embeddings\nconst items = $input.all();\nconst CHUNK_SIZE = 500;\nconst CHUNK_OVERLAP = 100;\n\n// Import axios for HTTP requests\nconst axios = require('axios');\n\nconst processedFiles = [];\nconst skippedFiles = [];\nconst errors = [];\nconst allBatches = [];\n\n// Process each file\nfor (const item of items) {\n  const fileInfo = item.json;\n  \n  // Skip if no content or error\n  if (!fileInfo.stdout || fileInfo.stderr) {\n    skippedFiles.push(fileInfo.filename);\n    console.log(`Skipping ${fileInfo.filename}: ${fileInfo.stderr || 'No content'}`);\n    continue;\n  }\n  \n  const content = fileInfo.stdout;\n  \n  // Skip very short content\n  if (!content || content.length < 50) {\n    skippedFiles.push(fileInfo.filename);\n    console.log(`Skipping ${fileInfo.filename}: Content too short`);\n    continue;\n  }\n  \n  processedFiles.push(fileInfo.filename);\n  \n  // Clean content\n  const cleanContent = content\n    .replace(/[\\r\\n]+/g, ' ')\n    .replace(/\\t+/g, ' ')\n    .replace(/\\s+/g, ' ')\n    .replace(/[^\\x20-\\x7E\\s]/g, '') // Remove non-printable chars\n    .trim();\n  \n  // Create chunks for this file\n  const fileChunks = [];\n  let start = 0;\n  let chunkIndex = 0;\n  \n  while (start < cleanContent.length && chunkIndex < 20) {\n    const end = Math.min(start + CHUNK_SIZE, cleanContent.length);\n    const chunkText = cleanContent.slice(start, end);\n    \n    if (chunkText.length < 50) break;\n    \n    fileChunks.push({\n      id: `${fileInfo.filename.replace(/[^a-zA-Z0-9]/g, '_')}_${chunkIndex}`,\n      text: chunkText,\n      metadata: {\n        source_file: fileInfo.filename,\n        file_path: fileInfo.displayPath,\n        chunk_index: chunkIndex,\n        chunk_length: chunkText.length,\n        file_size: fileInfo.sizeMB + ' MB',\n        modified: fileInfo.modified,\n        indexed_at: new Date().toISOString()\n      }\n    });\n    \n    chunkIndex++;\n    start += CHUNK_SIZE - CHUNK_OVERLAP;\n    \n    if (end === cleanContent.length) break;\n  }\n  \n  console.log(`Created ${fileChunks.length} chunks from ${fileInfo.filename}`);\n  \n  // Process chunks in batches of 10\n  const batchSize = 10;\n  for (let i = 0; i < fileChunks.length; i += batchSize) {\n    const batch = fileChunks.slice(i, i + batchSize);\n    allBatches.push(batch);\n  }\n}\n\nconsole.log(`Total files processed: ${processedFiles.length}`);\nconsole.log(`Total batches to process: ${allBatches.length}`);\n\n// Process each batch: generate embeddings and store in ChromaDB\nlet successCount = 0;\nlet batchNumber = 0;\n\nfor (const batch of allBatches) {\n  batchNumber++;\n  console.log(`Processing batch ${batchNumber}/${allBatches.length}`);\n  \n  try {\n    // Generate embeddings for each chunk in the batch\n    const embeddings = [];\n    \n    for (const chunk of batch) {\n      try {\n        // Simple POST request to Ollama for embeddings\n        const embeddingResponse = await axios.post(\n          'http://ollama:11434/api/embeddings',\n          {\n            model: 'nomic-embed-text',\n            prompt: chunk.text\n          },\n          {\n            headers: {\n              'Content-Type': 'application/json'\n            },\n            timeout: 30000\n          }\n        );\n        \n        if (embeddingResponse.data && embeddingResponse.data.embedding) {\n          embeddings.push(embeddingResponse.data.embedding);\n        } else {\n          console.log('No embedding in response');\n          embeddings.push(new Array(768).fill(0)); // Placeholder\n        }\n      } catch (embedError) {\n        console.error(`Embedding error: ${embedError.message}`);\n        embeddings.push(new Array(768).fill(0)); // Placeholder on error\n      }\n    }\n    \n    // Store batch in ChromaDB\n    const chromaPayload = {\n      ids: batch.map(c => c.id),\n      documents: batch.map(c => c.text),\n      metadatas: batch.map(c => c.metadata),\n      embeddings: embeddings\n    };\n    \n    try {\n      const chromaResponse = await fetch('http://chromadb:8000/api/v1/collections/local_docs/add', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify(chromaPayload)\n      });\n      \n      if (chromaResponse.ok) {\n        successCount += batch.length;\n        console.log(`  Stored ${batch.length} chunks successfully`);\n      } else {\n        const errorText = await chromaResponse.text();\n        errors.push(`ChromaDB error for batch ${batchNumber}: ${errorText}`);\n        console.error(`  Failed to store batch: ${errorText}`);\n      }\n    } catch (chromaError) {\n      errors.push(`ChromaDB error for batch ${batchNumber}: ${chromaError.message}`);\n      console.error(`  Failed to store batch: ${chromaError.message}`);\n    }\n    \n  } catch (error) {\n    errors.push(`Batch ${batchNumber} error: ${error.message}`);\n    console.error(`Error processing batch ${batchNumber}: ${error.message}`);\n  }\n}\n\n// Return summary\nreturn [{\n  json: {\n    success: successCount > 0,\n    message: `Successfully indexed ${successCount} document chunks`,\n    summary: {\n      files_processed: processedFiles.length,\n      files_skipped: skippedFiles.length,\n      total_chunks_stored: successCount,\n      total_batches: allBatches.length,\n      processed_files: processedFiles.slice(0, 10),\n      skipped_files: skippedFiles.slice(0, 10),\n      errors: errors.slice(0, 5),\n      timestamp: new Date().toISOString()\n    }\n  }\n}];"
      },
      "id": "process-and-store",
      "name": "Process and Store Documents",
      "type": "n8n-nodes-base.code",
      "position": [1020, 300],
      "typeVersion": 2,
      "notes": "Chunks documents, generates embeddings, and stores in ChromaDB"
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "List Files in G: Drive",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "List Files in G: Drive": {
      "main": [
        [
          {
            "node": "Parse File List",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse File List": {
      "main": [
        [
          {
            "node": "Read File Content",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Read File Content": {
      "main": [
        [
          {
            "node": "Process and Store Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "meta": {
    "templateId": "document-indexer-gdrive-simplified",
    "templateVersion": "3.0.0",
    "templateDescription": "Indexes documents from G: drive with simplified HTTP requests using axios in Code nodes"
  }
}